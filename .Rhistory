}else{ # append
prediction_CV <- bind_rows(prediction_CV , prediction_test)
}
# clean environment
rm(k , training.data , testing.data ,
predictor_train , predictor_test , response_train , response_test ,
prediction_test)
}
# evaluate  -------------------------------------------
evaluation_RF <- prediction_training %>%   # <-
full_join(prediction_CV ,
by = c("Station_name" , "NO2" , "Type_of_station" , "CV" , "month") ,
suffix = c("" , "_CV")) %>%
# calculate the indices from the observed and predicted values
summarize(MSE_training = mse(NO2 , predicted) ,
MSE_CV = mse(NO2 , predicted_CV) ,
MAE_training = mae(NO2 , predicted) ,
MAE_CV = mae(NO2 , predicted_CV) ,
R2_training = cor(NO2 , predicted)^2 ,
R2_CV = cor(NO2 , predicted_CV)^2 )
}
evaluation_RF
# =====================================
# embedded: regularization
# =====================================
included_var_all <- colnames(data_monthly)[!colnames(data_monthly) %in% columns_nonpredictor]
# train and evaluate model
{
included_var <- included_var_all
hyperparm_reglularization <- hyperparm_benchmark
hyperparm_reglularization["regularization"] <- 1
hyperparm_reglularization["regularization_factor"] <- 0.001
# full training set -------------------------------------------
training.data <- data_monthly %>% drop_na()
# make matrix
predictor_train <- training.data %>% select(one_of(included_var)) %>% as.matrix() # <-
response_train <- training.data %>% select(NO2) %>% as.matrix()
# define model
NN_define(hyperparm_reglularization , n_var = ncol(predictor_train))
# train model
NN %>%
fit(predictor_train , response_train ,
epoch = hyperparm_reglularization["epochs"] ,
batch_size = hyperparm_reglularization["batch.size"] ,
validation_split = 0.2 ,
verbose = TRUE)
# prediction
prediction_training <- training.data %>%
select(month , Station_name , NO2 , Type_of_station , CV) %>%
# prediction
mutate(predicted = predict(NN , predictor_train)[,1])
# clean environment
rm(training.data , predictor_train , response_train , NN)
# cross validation -------------------------------------------
for(k in as.factor(1:k_fold)){
# data preparation: partition
training.data <- data_monthly %>% filter(CV != k) %>% drop_na()
testing.data <- data_monthly %>% filter(CV == k) %>% drop_na()
# data preparation: make matrix
predictor_train <- training.data %>% select(all_of(included_var)) %>% as.matrix() # <-
response_train <- training.data %>% select(NO2) %>% as.matrix()
predictor_test <- testing.data %>% select(all_of(included_var)) %>% as.matrix() # <-
response_test <- testing.data %>% select(NO2) %>% as.matrix()
# define model
NN_define(hyperparm_benchmark , n_var = ncol(predictor_train))
# train model
NN %>%
fit(predictor_train , response_train ,
epoch = hyperparm_reglularization["epochs"] ,
batch_size = hyperparm_reglularization["batch.size"] ,
validation_split = 0.2 ,
verbose = FALSE)
# prediction
prediction_test <- testing.data %>%
select(month , Station_name , NO2 , Type_of_station , CV) %>%
# prediction
mutate(predicted = predict(NN , predictor_test)[,1])
# prediction data.frame
if(as.character(k) == "1"){
prediction_CV <- prediction_test
}else{ # append
prediction_CV <- bind_rows(prediction_CV , prediction_test)
}
# clean environment
rm(k , training.data , testing.data ,
predictor_train , predictor_test , response_train , response_test ,
prediction_test)
}
# evaluate  -------------------------------------------
evaluation_regularization <- prediction_training %>%   # <-
full_join(prediction_CV ,
by = c("Station_name" , "NO2" , "Type_of_station" , "CV" , "month") ,
suffix = c("" , "_CV")) %>%
# calculate the indices from the observed and predicted values
summarize(MSE_training = mse(NO2 , predicted) ,
MSE_CV = mse(NO2 , predicted_CV) ,
MAE_training = mae(NO2 , predicted) ,
MAE_CV = mae(NO2 , predicted_CV) ,
R2_training = cor(NO2 , predicted)^2 ,
R2_CV = cor(NO2 , predicted_CV)^2 )
}
# //////////////////////////////////////////////////////////////////////////
# evaluation of the different feature selection methods
# //////////////////////////////////////////////////////////////////////////
mget(ls(pattern = "^evaluation")) %>%
bind_rows() %>%
mutate(method = ls(pattern = "^evaluation" , envir=.GlobalEnv) %>%
str_remove("evaluation_"))
# //////////////////////////////////////////////////////////////////////////
# evaluation of the different feature selection methods
# //////////////////////////////////////////////////////////////////////////
mget(ls(pattern = "^evaluation")) %>%
bind_rows() %>%
mutate(method = ls(pattern = "^evaluation" , envir=.GlobalEnv) %>%
str_remove("evaluation_")) %>%
arrange(MAE_CV)
setdiff(included_var_garson , included_var_OGA)
# full training set -------------------------------------------
training.data <- data_monthly %>% drop_na()
# make matrix
set.seed(20210727)
predictor_train <- training.data %>%
select(-all_of(columns_nonpredictor)) %>%
# random-value variables
mutate(R1 = runif(n()) ,
R2 = runif(n()) ,
R3 = runif(n()) ) %>%
as.matrix()
response_train <- training.data %>%
select(NO2) %>%
as.matrix()
# model
NN_nnet <- nnet(
x = predictor_train ,
y = response_train ,
size = 50 , linout = TRUE , MaxNWts = 1e4
)
cor(NN_nnet$fitted.values , response_train)^2
# the functoin for determing relative importance
NN_nnet_varimp <- NeuralNetTools::garson(NN_nnet , bar_plot = FALSE) %>%
tibble(variables = row.names(.))
NN_nnet_varimp %>%
# re-order for visualization
mutate(variables = factor(variables , levels = variables[order(rel_imp)])) %>%
# random
mutate(class = ifelse(str_detect(variables , "R[123]") , "Random" , "Predictor variables")) %>%
# visualization
ggplot(aes(x = variables , y = rel_imp , fill = class)) +
geom_bar(stat = "identity") +
coord_flip() +
scale_fill_lancet() +
labs(x = "Variables" , y = "Variable importance" ,
title = "Screening of relevant predictor variables" ,
subtitle = "Relative importance of input variables in neural networks\n using Garson's algorithm") +
theme_bw() +
theme(axis.text.y = element_text(size = 6) , legend.position = "bottom")
included_var_garson <- NN_nnet_varimp %>%
filter(rel_imp > NN_nnet_varimp %>%
filter(str_detect(variables , "R[123]")) %>%
# the max importance of the random-value variables
summarize(rel_imp = max(rel_imp)) %>%
unlist) %>%
select(variables) %>%
filter(!str_detect(variables , "R[123]")) %>%
unlist %>% unname
# train and evaluate model
{
included_var <- included_var_garson
# full training set -------------------------------------------
training.data <- data_monthly %>% drop_na()
# make matrix
predictor_train <- training.data %>% select(one_of(included_var)) %>% as.matrix() # <-
response_train <- training.data %>% select(NO2) %>% as.matrix()
# define model
NN_define(hyperparm_benchmark , n_var = ncol(predictor_train))
# train model
NN %>%
fit(predictor_train , response_train ,
epoch = hyperparm_benchmark["epochs"] ,
batch_size = hyperparm_benchmark["batch.size"] ,
validation_split = 0.2 ,
verbose = TRUE)
# prediction
prediction_training <- training.data %>%
select(month , Station_name , NO2 , Type_of_station , CV) %>%
# prediction
mutate(predicted = predict(NN , predictor_train)[,1])
# clean environment
rm(training.data , predictor_train , response_train , NN)
# cross validation -------------------------------------------
for(k in as.factor(1:k_fold)){
# data preparation: partition
training.data <- data_monthly %>% filter(CV != k) %>% drop_na()
testing.data <- data_monthly %>% filter(CV == k) %>% drop_na()
# data preparation: make matrix
predictor_train <- training.data %>% select(all_of(included_var)) %>% as.matrix() # <-
response_train <- training.data %>% select(NO2) %>% as.matrix()
predictor_test <- testing.data %>% select(all_of(included_var)) %>% as.matrix() # <-
response_test <- testing.data %>% select(NO2) %>% as.matrix()
# define model
NN_define(hyperparm_benchmark , n_var = ncol(predictor_train))
# train model
NN %>%
fit(predictor_train , response_train ,
epoch = hyperparm_benchmark["epochs"] ,
batch_size = hyperparm_benchmark["batch.size"] ,
validation_split = 0.2 ,
verbose = FALSE)
# prediction
prediction_test <- testing.data %>%
select(month , Station_name , NO2 , Type_of_station , CV) %>%
# prediction
mutate(predicted = predict(NN , predictor_test)[,1])
# prediction data.frame
if(as.character(k) == "1"){
prediction_CV <- prediction_test
}else{ # append
prediction_CV <- bind_rows(prediction_CV , prediction_test)
}
# clean environment
rm(k , training.data , testing.data ,
predictor_train , predictor_test , response_train , response_test ,
prediction_test)
}
# evaluate  -------------------------------------------
evaluation_garson <- prediction_training %>%   # <-
full_join(prediction_CV ,
by = c("Station_name" , "NO2" , "Type_of_station" , "CV" , "month") ,
suffix = c("" , "_CV")) %>%
# calculate the indices from the observed and predicted values
summarize(MSE_training = mse(NO2 , predicted) ,
MSE_CV = mse(NO2 , predicted_CV) ,
MAE_training = mae(NO2 , predicted) ,
MAE_CV = mae(NO2 , predicted_CV) ,
R2_training = cor(NO2 , predicted)^2 ,
R2_CV = cor(NO2 , predicted_CV)^2 )
}
evaluation_garson
# //////////////////////////////////////////////////////////////////////////
# evaluation of the different feature selection methods
# //////////////////////////////////////////////////////////////////////////
mget(ls(pattern = "^evaluation")) %>%
bind_rows() %>%
mutate(method = ls(pattern = "^evaluation" , envir=.GlobalEnv) %>%
str_remove("evaluation_")) %>%
arrange(MAE_CV)
# full training set -------------------------------------------
training.data <- data_monthly %>% drop_na()
# make matrix
set.seed(20210727)
predictor_train <- training.data %>%
select(-all_of(columns_nonpredictor)) %>%
# random-value variables
mutate(R1 = runif(n()) ,
R2 = runif(n()) ,
R3 = runif(n()) ) %>%
as.matrix()
response_train <- training.data %>%
select(NO2) %>%
as.matrix()
# model
NN_nnet <- nnet(
x = predictor_train ,
y = response_train ,
size = 10 , linout = TRUE , MaxNWts = 1e4
)
cor(NN_nnet$fitted.values , response_train)^2
mae(NN_nnet$fitted.values , response_train)
# the functoin for determing relative importance
NN_nnet_varimp <- NeuralNetTools::garson(NN_nnet , bar_plot = FALSE) %>%
tibble(variables = row.names(.))
NN_nnet_varimp %>%
# re-order for visualization
mutate(variables = factor(variables , levels = variables[order(rel_imp)])) %>%
# random
mutate(class = ifelse(str_detect(variables , "R[123]") , "Random" , "Predictor variables")) %>%
# visualization
ggplot(aes(x = variables , y = rel_imp , fill = class)) +
geom_bar(stat = "identity") +
coord_flip() +
scale_fill_lancet() +
labs(x = "Variables" , y = "Variable importance" ,
title = "Screening of relevant predictor variables" ,
subtitle = "Relative importance of input variables in neural networks\n using Garson's algorithm") +
theme_bw() +
theme(axis.text.y = element_text(size = 6) , legend.position = "bottom")
# the functoin for determing relative importance
NN_nnet_varimp <- NeuralNetTools::garson(NN_nnet , bar_plot = FALSE) %>%
tibble(variables = row.names(.))
NN_nnet_varimp %>%
# re-order for visualization
mutate(variables = factor(variables , levels = variables[order(rel_imp)])) %>%
# random
mutate(class = ifelse(str_detect(variables , "R[123]") , "Random" , "Predictor variables")) %>%
# visualization
ggplot(aes(x = variables , y = rel_imp , fill = class)) +
geom_bar(stat = "identity") +
coord_flip() +
scale_fill_lancet() +
labs(x = "Variables" , y = "Variable importance" ,
title = "Screening of relevant predictor variables" ,
subtitle = "Relative importance of input variables in neural networks\n using Garson's algorithm") +
theme_bw() +
theme(axis.text.y = element_text(size = 6) , legend.position = "bottom")
included_var_garson <- NN_nnet_varimp %>%
filter(rel_imp > NN_nnet_varimp %>%
filter(str_detect(variables , "R[123]")) %>%
# the max importance of the random-value variables
summarize(rel_imp = max(rel_imp)) %>%
unlist) %>%
select(variables) %>%
filter(!str_detect(variables , "R[123]")) %>%
unlist %>% unname
# train and evaluate model
{
included_var <- included_var_garson
# full training set -------------------------------------------
training.data <- data_monthly %>% drop_na()
# make matrix
predictor_train <- training.data %>% select(one_of(included_var)) %>% as.matrix() # <-
response_train <- training.data %>% select(NO2) %>% as.matrix()
# define model
NN_define(hyperparm_benchmark , n_var = ncol(predictor_train))
# train model
NN %>%
fit(predictor_train , response_train ,
epoch = hyperparm_benchmark["epochs"] ,
batch_size = hyperparm_benchmark["batch.size"] ,
validation_split = 0.2 ,
verbose = TRUE)
# prediction
prediction_training <- training.data %>%
select(month , Station_name , NO2 , Type_of_station , CV) %>%
# prediction
mutate(predicted = predict(NN , predictor_train)[,1])
# clean environment
rm(training.data , predictor_train , response_train , NN)
# cross validation -------------------------------------------
for(k in as.factor(1:k_fold)){
# data preparation: partition
training.data <- data_monthly %>% filter(CV != k) %>% drop_na()
testing.data <- data_monthly %>% filter(CV == k) %>% drop_na()
# data preparation: make matrix
predictor_train <- training.data %>% select(all_of(included_var)) %>% as.matrix() # <-
response_train <- training.data %>% select(NO2) %>% as.matrix()
predictor_test <- testing.data %>% select(all_of(included_var)) %>% as.matrix() # <-
response_test <- testing.data %>% select(NO2) %>% as.matrix()
# define model
NN_define(hyperparm_benchmark , n_var = ncol(predictor_train))
# train model
NN %>%
fit(predictor_train , response_train ,
epoch = hyperparm_benchmark["epochs"] ,
batch_size = hyperparm_benchmark["batch.size"] ,
validation_split = 0.2 ,
verbose = FALSE)
# prediction
prediction_test <- testing.data %>%
select(month , Station_name , NO2 , Type_of_station , CV) %>%
# prediction
mutate(predicted = predict(NN , predictor_test)[,1])
# prediction data.frame
if(as.character(k) == "1"){
prediction_CV <- prediction_test
}else{ # append
prediction_CV <- bind_rows(prediction_CV , prediction_test)
}
# clean environment
rm(k , training.data , testing.data ,
predictor_train , predictor_test , response_train , response_test ,
prediction_test)
}
# evaluate  -------------------------------------------
evaluation_garson <- prediction_training %>%   # <-
full_join(prediction_CV ,
by = c("Station_name" , "NO2" , "Type_of_station" , "CV" , "month") ,
suffix = c("" , "_CV")) %>%
# calculate the indices from the observed and predicted values
summarize(MSE_training = mse(NO2 , predicted) ,
MSE_CV = mse(NO2 , predicted_CV) ,
MAE_training = mae(NO2 , predicted) ,
MAE_CV = mae(NO2 , predicted_CV) ,
R2_training = cor(NO2 , predicted)^2 ,
R2_CV = cor(NO2 , predicted_CV)^2 )
}
evaluation_garson
# full training set -------------------------------------------
training.data <- data_monthly %>% drop_na()
# full training set -------------------------------------------
training.data <- data_monthly %>% drop_na()
# full training set -------------------------------------------
training.data <- data_monthly %>% drop_na()
# make matrix
set.seed(20210727)
predictor_train <- training.data %>%
select(-all_of(columns_nonpredictor)) %>%
# random-value variables
mutate(R1 = runif(n()) ,
R2 = runif(n()) ,
R3 = runif(n()) ) %>%
as.matrix()
response_train <- training.data %>%
select(NO2) %>%
as.matrix()
# model
NN_nnet <- nnet(
x = predictor_train ,
y = response_train ,
size = 20 , linout = TRUE , MaxNWts = 1e4
)
cor(NN_nnet$fitted.values , response_train)^2
# the functoin for determing relative importance
NN_nnet_varimp <- NeuralNetTools::garson(NN_nnet , bar_plot = FALSE) %>%
tibble(variables = row.names(.))
NN_nnet_varimp %>%
# re-order for visualization
mutate(variables = factor(variables , levels = variables[order(rel_imp)])) %>%
# random
mutate(class = ifelse(str_detect(variables , "R[123]") , "Random" , "Predictor variables")) %>%
# visualization
ggplot(aes(x = variables , y = rel_imp , fill = class)) +
geom_bar(stat = "identity") +
coord_flip() +
scale_fill_lancet() +
labs(x = "Variables" , y = "Variable importance" ,
title = "Screening of relevant predictor variables" ,
subtitle = "Relative importance of input variables in neural networks\n using Garson's algorithm") +
theme_bw() +
theme(axis.text.y = element_text(size = 6) , legend.position = "bottom")
included_var_garson <- NN_nnet_varimp %>%
filter(rel_imp > NN_nnet_varimp %>%
filter(str_detect(variables , "R[123]")) %>%
# the max importance of the random-value variables
summarize(rel_imp = max(rel_imp)) %>%
unlist) %>%
select(variables) %>%
filter(!str_detect(variables , "R[123]")) %>%
unlist %>% unname
# train and evaluate model
{
included_var <- included_var_garson
# full training set -------------------------------------------
training.data <- data_monthly %>% drop_na()
# make matrix
predictor_train <- training.data %>% select(one_of(included_var)) %>% as.matrix() # <-
response_train <- training.data %>% select(NO2) %>% as.matrix()
# define model
NN_define(hyperparm_benchmark , n_var = ncol(predictor_train))
# train model
NN %>%
fit(predictor_train , response_train ,
epoch = hyperparm_benchmark["epochs"] ,
batch_size = hyperparm_benchmark["batch.size"] ,
validation_split = 0.2 ,
verbose = TRUE)
# prediction
prediction_training <- training.data %>%
select(month , Station_name , NO2 , Type_of_station , CV) %>%
# prediction
mutate(predicted = predict(NN , predictor_train)[,1])
# clean environment
rm(training.data , predictor_train , response_train , NN)
# cross validation -------------------------------------------
for(k in as.factor(1:k_fold)){
# data preparation: partition
training.data <- data_monthly %>% filter(CV != k) %>% drop_na()
testing.data <- data_monthly %>% filter(CV == k) %>% drop_na()
# data preparation: make matrix
predictor_train <- training.data %>% select(all_of(included_var)) %>% as.matrix() # <-
response_train <- training.data %>% select(NO2) %>% as.matrix()
predictor_test <- testing.data %>% select(all_of(included_var)) %>% as.matrix() # <-
response_test <- testing.data %>% select(NO2) %>% as.matrix()
# define model
NN_define(hyperparm_benchmark , n_var = ncol(predictor_train))
# train model
NN %>%
fit(predictor_train , response_train ,
epoch = hyperparm_benchmark["epochs"] ,
batch_size = hyperparm_benchmark["batch.size"] ,
validation_split = 0.2 ,
verbose = FALSE)
# prediction
prediction_test <- testing.data %>%
select(month , Station_name , NO2 , Type_of_station , CV) %>%
# prediction
mutate(predicted = predict(NN , predictor_test)[,1])
# prediction data.frame
if(as.character(k) == "1"){
prediction_CV <- prediction_test
}else{ # append
prediction_CV <- bind_rows(prediction_CV , prediction_test)
}
# clean environment
rm(k , training.data , testing.data ,
predictor_train , predictor_test , response_train , response_test ,
prediction_test)
}
# evaluate  -------------------------------------------
evaluation_garson <- prediction_training %>%   # <-
full_join(prediction_CV ,
by = c("Station_name" , "NO2" , "Type_of_station" , "CV" , "month") ,
suffix = c("" , "_CV")) %>%
# calculate the indices from the observed and predicted values
summarize(MSE_training = mse(NO2 , predicted) ,
MSE_CV = mse(NO2 , predicted_CV) ,
MAE_training = mae(NO2 , predicted) ,
MAE_CV = mae(NO2 , predicted_CV) ,
R2_training = cor(NO2 , predicted)^2 ,
R2_CV = cor(NO2 , predicted_CV)^2 )
}
evaluation_garson
NN_Lasso$feature_importances_

)
# variables importance using Garson's algorithm
NN_screen_importance <- NeuralNetTools::garson(NN_screen , bar_plot = FALSE) %>%
tibble(variables = row.names(.))
# variable selection
included_var_garson <- NN_screen_importance %>%
filter(rel_imp > NN_screen_importance %>%
filter(str_detect(variables , "R[123]")) %>%
# the max importance of the random-value variables
summarize(rel_imp = max(rel_imp)) %>%
unlist) %>%
select(variables) %>%
filter(!str_detect(variables , "R[123]")) %>%
unlist %>% unname
# no selection
included_var_all <- colnames(data_annual)[!colnames(data_annual) %in% columns_nonpredictor]
# =====================================
# hyperparameters
# =====================================
hyperparm_vector <- hyper_evaluation %>%
arrange(MAE_CV) %>%
dplyr::slice(1) %>%
select(-contains("_training") , -contains("_CV")) %>%
unlist
# =====================================
# model with the full training set
# =====================================
if(is.na(hyperparm_vector["garson_selection"])){
included_var <- included_var_all
}else if(hyperparm_vector["garson_selection"] == 1){
included_var <- included_var_garson
}
{ # data preparation
training.data <- data_annual %>%
drop_na()
predictor_train <- training.data %>%
select(all_of(included_var)) %>%
as.matrix()
response_train <- training.data %>%
select(NO2) %>%
as.matrix()
}
# define model
set_random_seed(1010) # reproducibility for Keras
NN_define(hyperparm_vector = hyperparm_vector , n_var = ncol(predictor_train))
# train model
NN %>%
fit(predictor_train , response_train ,
epoch = hyperparm_vector["epochs"] ,
batch_size = hyperparm_vector["batch.size"] ,
validation_split = 0.2 ,
verbose = TRUE)
# prediction
NN_prediction_training <- training.data %>%
select(Station_name , NO2 , Type_of_station , X , Y) %>%
# prediction
mutate(predicted = predict(NN , predictor_train)[,1])
# save full-model NN
out_dirpath_model <- "3_results/output-model/model_annual"
save_model_hdf5(
NN ,
sprintf("%s/%s_%s.hdf" , out_dirpath_model , model_abbr , SAT_product)
)
# =====================================
library(readr)
library(sf)
library(dplyr) ; library(tidyr)
library(ggplot2) ; library(ggsci) ; library(ggthemes)
library(lubridate) ; library(stringr)
library(spdep)
library(Metrics)
library(lightgbm)
# =====================================
# load datasets
# =====================================
# training data
data_annual_raw <- read_csv("1_data/processed/cleaned/extracted/annual_scaled.csv")
# cross validation
{
in_filepath_CV <- list.files("1_data/processed/cleaned/extracted/" , "CV.shp$" , full.names = TRUE)
sites_CV <- st_read(in_filepath_CV) %>%
rename(Station_name = Station)
k_fold <- in_filepath_CV %>%
str_extract("\\d+-fold") %>% str_extract("\\d+") %>%
as.integer()
}
# implement the CV design in the training data
data_annual_raw <- data_annual_raw %>%
inner_join(sites_CV , by = "Station_name") %>%
select(-geometry)
# non-predictor columns
columns_nonpredictor <- c("Station_name" , "NO2" , "Type_of_zone" , "Type_of_station" ,
"Altitude" , "Canton_ID" , "Canton_name" , "X" , "Y" ,
"CV" , "spatial_CV")
# //////////////////////////////////////////////////////////////////////////
# naming the model
# //////////////////////////////////////////////////////////////////////////
model_name <- "Light gradient boosting machine"
model_abbr <- "LGB"
SAT_product <- c("OMI" , "TROPOMI")[1]
# //////////////////////////////////////////////////////////////////////////
# data preparation for LightGBM
# //////////////////////////////////////////////////////////////////////////
if(SAT_product == "OMI"){
data_annual <- data_annual_raw %>%
# OMI
select(-TROPOMI_NO2, -ends_with("12H")) %>%
drop_na()
}else if(SAT_product == "TROPOMI") {
data_annual <- data_annual_raw %>%
# TROPOMI
select(-OMI_NO2, -ends_with("15H")) %>%
drop_na()
}
# =====================================
# response
# =====================================
response_full <- data_annual %>%
select(NO2) %>%
as.matrix()
# =====================================
# predictor
# screening of important predictor variables
# =====================================
set.seed(123)
lgb_screen <- lightgbm(
data = data_annual %>%
select(-all_of(columns_nonpredictor)) %>%
# random-value variables
mutate(R1 = runif(n()) ,
R2 = runif(n()) ,
R3 = runif(n()) ) %>%
# matrix
as.matrix() %>%
# lgb.Dataset object
lgb.Dataset(data = . , label = response_full) ,
# default
params = list(learning_rate = 0.1 , num_leaves = 31) ,
objective = "regression" ,
boosting_type = "gbdt" ,
nrounds = 150
)
# importance
lgb_screen_importance <- lgb_screen %>%
lgb.importance(model = .) %>%
as_tibble()
# visualization
lgb_screen_importance %>%
mutate(class = ifelse(str_detect(Feature , "R[123]") , "Random" , "Predictor variables")) %>%
# reorder for visualization
mutate(Feature = factor(Feature , levels = Feature[order(Gain)])) %>%
# visualization
ggplot(aes(x = Feature , y = Gain , fill = class)) +
geom_bar(stat = "identity") +
coord_flip() +
scale_fill_lancet() +
labs(x = "Variables" , y = "Variable importance (Gain)" , fill = "" ,
title = "Screening of relevant predictor variables" ,
subtitle = sprintf("The variable importance of LightGBM (%s)" , SAT_product)) +
theme_bw() +
theme(axis.text.y = element_text(size = 4) , legend.position = "bottom")
# exclude not-important variables
included_var <- lgb_screen_importance %>%
filter(Gain > lgb_screen_importance %>%
filter(str_detect(Feature , "R[123]")) %>%
# the max importance of the random-value variables
summarize(Gain = max(Gain)) %>%
unlist) %>%
select(Feature) %>%
unlist %>% unname()
# =====================================
# predictors (as a matrix)
# =====================================
predictor_full <- data_annual %>%
select(all_of(included_var)) %>%
as.matrix()
# //////////////////////////////////////////////////////////////////////////
# final model
# //////////////////////////////////////////////////////////////////////////
# =====================================
# hyperparameters from grid search
# =====================================
hyper_grid <- read_csv("3_results/output-data/model_annual/LGB_grid-search/hyper_evaluation.csv")
hyperparm_final <- hyper_grid %>%
arrange(min_RMSE) %>%
dplyr::slice(1) %>%
select(learning_rate , max_depth , num_leaves , bagging_fraction , feature_fraction , boosting_type) %>%
as.list()
# learning_rate max_depth num_leaves bagging_fraction feature_fraction boosting_type
#           0.3         1         25              0.5             0.75 dart
nrounds <- hyper_grid %>%
arrange(min_RMSE) %>%
dplyr::slice(1) %>%
select(optimal_trees) %>%
unlist
# =====================================
# model with the full training set
# =====================================
set.seed(123)
lgb_final_full <- lightgbm(
params = hyperparm_final ,
data = predictor_full ,
label = response_full ,
nrounds = nrounds ,
objective = "regression" ,
eval = "rmse" ,
task = "prediction" ,
save_name = NA ,
verbose = -1 # silent
)
lgb_final_full
# =====================================
# export model
# =====================================
{
out_dirpath_model <- "3_results/output-model/model_annual"
if(!dir.exists(out_dirpath_model)) dir.create(out_dirpath_model)
lgb.save(
lgb_final_full ,
filename = sprintf("%s/%s_%s.txt" , out_dirpath_model , model_abbr , SAT_product)
)
# saveRDS.lgb.Booster(lgb_final_full ,
#                     file = sprintf("%s/%s_%s.rds" , out_dirpath_model , model_abbr , SAT_product))
}
sprintf("%s/%s_%s.txt" , out_dirpath_model , model_abbr , SAT_product)
SAT_product <- c("OMI" , "TROPOMI")[2]
# //////////////////////////////////////////////////////////////////////////
# data preparation for LightGBM
# //////////////////////////////////////////////////////////////////////////
if(SAT_product == "OMI"){
data_annual <- data_annual_raw %>%
# OMI
select(-TROPOMI_NO2, -ends_with("12H")) %>%
drop_na()
}else if(SAT_product == "TROPOMI") {
data_annual <- data_annual_raw %>%
# TROPOMI
select(-OMI_NO2, -ends_with("15H")) %>%
drop_na()
}
# =====================================
# response
# =====================================
response_full <- data_annual %>%
select(NO2) %>%
as.matrix()
# =====================================
# predictor
# screening of important predictor variables
# =====================================
set.seed(123)
lgb_screen <- lightgbm(
data = data_annual %>%
select(-all_of(columns_nonpredictor)) %>%
# random-value variables
mutate(R1 = runif(n()) ,
R2 = runif(n()) ,
R3 = runif(n()) ) %>%
# matrix
as.matrix() %>%
# lgb.Dataset object
lgb.Dataset(data = . , label = response_full) ,
# default
params = list(learning_rate = 0.1 , num_leaves = 31) ,
objective = "regression" ,
boosting_type = "gbdt" ,
nrounds = 150
)
# importance
lgb_screen_importance <- lgb_screen %>%
lgb.importance(model = .) %>%
as_tibble()
# exclude not-important variables
included_var <- lgb_screen_importance %>%
filter(Gain > lgb_screen_importance %>%
filter(str_detect(Feature , "R[123]")) %>%
# the max importance of the random-value variables
summarize(Gain = max(Gain)) %>%
unlist) %>%
select(Feature) %>%
unlist %>% unname()
# =====================================
# predictors (as a matrix)
# =====================================
predictor_full <- data_annual %>%
select(all_of(included_var)) %>%
as.matrix()
# //////////////////////////////////////////////////////////////////////////
# final model
# //////////////////////////////////////////////////////////////////////////
# =====================================
# hyperparameters from grid search
# =====================================
hyper_grid <- read_csv("3_results/output-data/model_annual/LGB_grid-search/hyper_evaluation.csv")
hyperparm_final <- hyper_grid %>%
arrange(min_RMSE) %>%
dplyr::slice(1) %>%
select(learning_rate , max_depth , num_leaves , bagging_fraction , feature_fraction , boosting_type) %>%
as.list()
# learning_rate max_depth num_leaves bagging_fraction feature_fraction boosting_type
#           0.3         1         25              0.5             0.75 dart
nrounds <- hyper_grid %>%
arrange(min_RMSE) %>%
dplyr::slice(1) %>%
select(optimal_trees) %>%
unlist
# =====================================
# model with the full training set
# =====================================
set.seed(123)
lgb_final_full <- lightgbm(
params = hyperparm_final ,
data = predictor_full ,
label = response_full ,
nrounds = nrounds ,
objective = "regression" ,
eval = "rmse" ,
task = "prediction" ,
save_name = NA ,
verbose = -1 # silent
)
# =====================================
# export model
# =====================================
{
out_dirpath_model <- "3_results/output-model/model_annual"
if(!dir.exists(out_dirpath_model)) dir.create(out_dirpath_model)
lgb.save(
lgb_final_full ,
filename = sprintf("%s/%s_%s.txt" , out_dirpath_model , model_abbr , SAT_product)
)
}
library(readr)
library(dplyr) ; library(tidyr)
library(ggplot2) ; library(ggsci) ; library(ggthemes) ; library(cowplot)
library(lubridate) ; library(stringr)
source("2_scripts/utils_model-eval.R")
tempres <- "daily"
data_annual_aggregated
# visualization
data_annual_aggregated %>%
select(model , product , NO2 , starts_with("predicted"))
# visualization
data_annual_aggregated %>%
select(model , product , Station_name , NO2 , starts_with("predicted"))
# visualization
data_annual_aggregated %>%
select(model , product , Station_name , NO2 , starts_with("predicted")) %>%
ggplot(aes(x = predicted_temporalCV , y = NO2)) +
geom_point() +
facet_wrap(~model + product)
# visualization
data_annual_aggregated %>%
select(model , product , Station_name , NO2 , starts_with("predicted")) %>%
ggplot(aes(x = predicted_temporalCV , y = NO2)) +
geom_point(shape = 1) +
facet_wrap(~model + product)
model_compare_tidy_aggregated
model_compare_tidy_aggregated %>%
select(model , product ,
training_R2 , training_RMSE ,
CV_R2 , CV_RMSE ,
spatialCV_R2 , spatialCV_RMSE)
# tidy table with R2, RMSE, Moran's I
model_compare_tidy_aggregated %>%
select(model , product ,
training_R2 , training_RMSE ,
CV_R2 , CV_RMSE ,
spatialCV_R2 , spatialCV_RMSE) %>%
write_csv(sprintf("3_results/output-data/model_%s/model_%s_aggregated_indices_1.csv" , tempres , tempres))
# tidy table with slope and intercept
model_compare_tidy_aggregated %>%
select(model , product ,
training_slope , training_intercept ,
CV_slope , CV_intercept ,
spatialCV_slope , spatialCV_intercept) %>%
write_csv(sprintf("3_results/output-data/model_%s/model_%s_aggregated_indices_2.csv" , tempres , tempres))
library(readr) ; library(vroom)
library(sf)
library(dplyr) ; library(tidyr)
library(ggplot2) ; library(ggsci) ; library(ggthemes)
library(lubridate) ; library(stringr)
library(spdep)
library(Metrics)
library(ranger)
source("2_scripts/utils_model-eval.R")
# =====================================
# load datasets
# =====================================
# training data
data_daily_raw <- read_csv("1_data/processed/cleaned/extracted/daily_scaled.csv")
# cross validation
{
in_filepath_CV <- list.files("1_data/processed/cleaned/extracted/" , "CV.shp$" , full.names = TRUE)
sites_CV <- st_read(in_filepath_CV) %>%
rename(Station_name = Station)
k_fold <- in_filepath_CV %>%
str_extract("\\d+-fold") %>% str_extract("\\d+") %>%
as.integer()
}
# implement the CV design in the training data
data_daily_raw <- data_daily_raw %>%
inner_join(sites_CV , by = "Station_name") %>%
select(-geometry)
# non-predictor columns
columns_nonpredictor <- c("Station_name" , "Type_of_zone" , "Type_of_station" ,
"Altitude" , "Canton_ID" , "Canton_name" , "X" , "Y" ,
"CV" , "spatial_CV")
# //////////////////////////////////////////////////////////////////////////
# naming the model
# //////////////////////////////////////////////////////////////////////////
model_name <- "Random forest"
model_abbr <- "RF"
SAT_product <- c("OMI" , "TROPOMI")[2]
# random forest hyperparameters
hyper_grid <- read_csv("3_results/output-data/model_daily/RF_grid-search/hyper_evaluation.csv")
hyperparm_final <- hyper_grid %>%
arrange(-CV_R2) %>%
slice(1) %>%
as.list()
data_daily_raw
data_daily_raw %>%
select(-TROPOMI_NO2 , -ends_with("_12H")) %>%
select(date) %>%
mutate(week = wday(date))
data_daily_raw %>%
select(-TROPOMI_NO2 , -ends_with("_12H")) %>%
select(date) %>%
mutate(week = wday(date)) %>% distinct()
# for the OMI model: exclude TROPOMI and meteorological variables at 12H
data_daily <- data_daily_raw %>%
select(-TROPOMI_NO2 , -ends_with("_12H")) %>%
mutate(weekday = ifelse(wday(date) %in% c(1:5) , 1 , 0))
SAT_product
# =====================================
# subset data: satellite-product
# =====================================
if(SAT_product == "OMI"){
# for the OMI model: exclude TROPOMI and meteorological variables at 12H
data_daily <- data_daily_raw %>%
select(-TROPOMI_NO2 , -ends_with("_12H")) %>%
mutate(weekday = ifelse(wday(date) %in% c(1:5) , 1 , 0))
}else if(SAT_product == "TROPOMI"){
# for the TROPOMI model: exclude OMI and meteorological variables at 15H
data_daily <- data_daily_raw %>%
select(-OMI_NO2 , -ends_with("_15H")) %>%
mutate(weekday = ifelse(wday(date) %in% c(1:5) , 1 , 0))
}
data_daily
# =====================================
# screening of important variables
# =====================================
set.seed(1010)
rf_screen <- ranger(NO2 ~ . ,
data = data_daily %>%
# drop non-predictor columns
select(-all_of(columns_nonpredictor)) %>%
drop_na() %>%
# random-value variables
mutate(R1 = runif(n()) ,
R2 = runif(n()) ,
R3 = runif(n()) ) ,
importance = "impurity" ,
keep.inbag = TRUE ,
num.trees = 3000 ,
mtry = 30 )
rf_screen_importance <- rf_screen$variable.importance %>%
sort(decreasing = TRUE) %>%
as.list() %>%
as_tibble() %>%
pivot_longer(cols = everything() , names_to = "variable" , values_to = "importance")
# variable importance screening
rf_screen_importance %>%
mutate(class = ifelse(str_detect(variable , "R[123]") , "Random" , "Predictor variables")) %>%
# reorder for visualization
mutate(variable = factor(variable , levels = variable[order(importance)])) %>%
# visualization
ggplot(aes(x = variable , y = importance , fill = class)) +
geom_bar(stat = "identity") +
coord_flip() +
scale_fill_lancet() +
labs(x = "Variables" , y = "Variable importance" , fill = "" ,
title = "Screening of relevant predictor variables" ,
subtitle = sprintf("The variable importance of the random forest model (%s)" , SAT_product)) +
theme_bw() +
theme(axis.text.y = element_text(size = 4) , legend.position = "bottom")
rf_screen_importance %>% arrange()
rf_screen_importance %>% arrange(-importance)
rf_screen_importance %>% arrange(-importance) %>% View
# =====================================
# screening of important variables
# =====================================
set.seed(1010)
rf_screen <- ranger(NO2 ~ . ,
data = data_daily %>%
# drop non-predictor columns
select(-all_of(columns_nonpredictor)) %>%
drop_na() %>%
# random-value variables
mutate(R1 = runif(n()) ,
R2 = runif(n()) ,
R3 = runif(n()) ) ,
importance = "impurity" ,
keep.inbag = TRUE ,
num.trees = 3000 ,
mtry = 10 )
rf_screen_importance <- rf_screen$variable.importance %>%
sort(decreasing = TRUE) %>%
as.list() %>%
as_tibble() %>%
pivot_longer(cols = everything() , names_to = "variable" , values_to = "importance")
rf_screen_importance %>% arrange(-importance) %>% View

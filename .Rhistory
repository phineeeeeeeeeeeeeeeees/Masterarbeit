response_test <- testing.data %>%
select(NO2) %>%
as.matrix()
# define model
set.seed(123)
xgb_train <- xgboost(
data = predictor_train ,
label = response_train ,
params = hyperparm_final ,
nrounds = nrounds ,
objective = "reg:squarederror" ,
verbose = FALSE ,  # silent
early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
# prediction
prediction_test <- testing.data %>%
select(Station_name , NO2 , Type_of_station , CV) %>%
# prediction
mutate(predicted = predict(xgb_train , predictor_test))
# prediction data.frame
if(as.character(k) == "1"){
xgb_final_prediction_CV <- prediction_test # <-
}else{ # append
xgb_final_prediction_CV <- bind_rows(xgb_final_prediction_CV , prediction_test)
}
# clean environment
rm(xgb_train)
rm(k , training.data , testing.data , predictor_train , predictor_test , response_train , response_test , prediction_test)
}
# spatial CV
for(k in as.factor(1:k_fold)){
# data preparation: partition
training.data <- data_annual %>%
filter(spatial_CV != k)
testing.data <- data_annual %>%
filter(spatial_CV == k)
# data preparation: make matrix
predictor_train <- training.data %>%
select(all_of(included_var)) %>%
as.matrix()
response_train <- training.data %>%
select(NO2) %>%
as.matrix()
predictor_test <- testing.data %>%
select(all_of(included_var)) %>%
as.matrix()
response_test <- testing.data %>%
select(NO2) %>%
as.matrix()
# define model
set.seed(123)
xgb_train <- xgboost(
data = predictor_train ,
label = response_train ,
params = hyperparm_final ,
nrounds = nrounds ,
objective = "reg:squarederror" ,
verbose = FALSE ,  # silent
early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
# prediction
prediction_test <- testing.data %>%
select(Station_name , NO2 , Type_of_station , spatial_CV) %>%
# prediction
mutate(predicted = predict(xgb_train , predictor_test))
# prediction data.frame
if(as.character(k) == "1"){
xgb_final_prediction_CV_sp <- prediction_test # <-
}else{ # append
xgb_final_prediction_CV_sp <- bind_rows(xgb_final_prediction_CV_sp , prediction_test)
}
# clean environment
rm(xgb_train)
rm(k , training.data , testing.data , predictor_train , predictor_test , response_train , response_test , prediction_test)
}
# combine the prediction of the two CV
xgb_final_prediction_CV <- xgb_final_prediction_CV %>%
left_join(xgb_final_prediction_CV_sp ,
by = c("Station_name" , "NO2" , "Type_of_station") ,
suffix = c("_CV" , "_spatialCV"))
# //////////////////////////////////////////////////////////////////////////
# model evaluation
# //////////////////////////////////////////////////////////////////////////
# =====================================
# observed, predicted, residuals
# =====================================
xgb_final_prediction <- data_annual %>%
select(Station_name , NO2 , Type_of_station , X , Y) %>%
# prediction
mutate(predicted = predict(xgb_final_full , predictor_full)) %>%
# CV-prediction
full_join(xgb_final_prediction_CV ,
by = c("Station_name" , "NO2" , "Type_of_station")) %>%
# residuals
mutate(residual = NO2 - predicted ,
residual_CV = NO2 - predicted_CV ,
residual_spatialCV = NO2 - predicted_spatialCV)
# =====================================
# performance indices
# =====================================
# model performance indices as a data.frame
xgb_final_indices <- xgb_final_prediction %>%
eval_performance_indices()
# =====================================
# variable importance
# =====================================
# Gain: the relative contribution of the corresponding feature to the model
# calculated by taking each feature’s contribution for each tree in the model.
# This is synonymous with gbm’s relative.influence
xgb_final_importance <- xgb_final_full %>%
xgb.importance(model = .)
# =====================================
# visualization
# =====================================
out_dirpath_plots <- sprintf("3_results/output-graph/model_annual/%s" , model_abbr)
if(!dir.exists(out_dirpath_plots)) dir.create(out_dirpath_plots)
# predicted <-> observed
plot_obs_pred(xgb_final_prediction , # <-
sprintf("%s (%s)" , str_to_title(model_name) , SAT_product))
save_plot(
sprintf("%s/obs-pred_%s_%s.png" , out_dirpath_plots , model_abbr , SAT_product) ,
plot = last_plot() ,
base_width = 5 , base_height = 3.5
)
# residual diagnostic plots
plot_resid(xgb_final_prediction , # <-
title_text = sprintf("%s (%s)" , str_to_title(model_name) , SAT_product))
save_plot(
sprintf("%s/residuals_%s_%s.png" , out_dirpath_plots , model_abbr , SAT_product) ,
plot = last_plot() ,
base_width = 7.8 , base_height = 6
)
# variable importance
xgb_final_importance %>%
arrange(-Gain) %>%
# re-order for visualization
mutate(Feature = factor(Feature , levels = Feature[order(Gain)])) %>%
# dplyr::slice(1:30) %>%
ggplot(aes(x = Feature , y = Gain)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(x = "Variables" , y = "Gain" ,
title = sprintf("GBM variable importance (with %s)" , SAT_product) ,
subtitle = "Gain: the relative contribution of the corresponding \nfeature to the model") +
theme_bw() +
theme(axis.text.y = element_text(size = 6))
save_plot(
sprintf("%s/importance_%s_%s.png" , out_dirpath_plots , model_abbr , SAT_product) ,
plot = last_plot() ,
base_width = 6 , base_height = 6
)
# =====================================
# spatial autocorrelation of the residuals
# =====================================
moran_df <- eval_resid_moran(xgb_final_prediction)
# visualization
plot_resid_map(xgb_final_prediction , # <-
sprintf("%s (%s)" , str_to_title(model_name) , SAT_product))
save_plot(
sprintf("%s/residual-map_%s_%s.png" , out_dirpath_plots , model_abbr , SAT_product) ,
plot = last_plot() ,
base_width = 6 , base_height = 4
)
# =====================================
# export datasets
# =====================================
{
# export the predicted values
out_dirpath_predicted <- "3_results/output-data/model_annual/observed-predicted"
if(!dir.exists(out_dirpath_predicted)) dir.create(out_dirpath_predicted)
xgb_final_prediction %>% # <-
mutate(model = model_abbr , product = SAT_product) %>%
write_csv(sprintf("%s/%s_%s.csv" , out_dirpath_predicted , model_abbr , SAT_product))
# export the model performance indices
out_dirpath_indices <- "3_results/output-data/model_annual/indices"
if(!dir.exists(out_dirpath_indices)) dir.create(out_dirpath_indices)
xgb_final_indices %>% # <-
mutate(model = model_abbr , product = SAT_product) %>%
write_csv(sprintf("%s/%s_%s.csv" , out_dirpath_indices , model_abbr , SAT_product))
# Moran's I
out_dirpath_Moran <- "3_results/output-data/model_annual/Moran"
if(!dir.exists(out_dirpath_Moran)) dir.create(out_dirpath_Moran)
moran_df %>%
pivot_longer(cols = everything()) %>%
mutate(model = model_abbr , product = SAT_product) %>%
write_csv(sprintf("%s/%s_%s.csv" , out_dirpath_Moran , model_abbr , SAT_product))
}
# =====================================
# export model
# =====================================
{
out_dirpath_model <- "3_results/output-model/model_annual"
if(!dir.exists(out_dirpath_model)) dir.create(out_dirpath_model)
saveRDS(xgb_final_full , # <-
file = sprintf("%s/%s_%s.rds" , out_dirpath_model , model_abbr , SAT_product))
}
renv::restore()
install.packages("foreign")
renv::restore()
library(foreign)
renv::restore()
renv::restore(packages = -foreign)
renv::restore(packages = -"foreign")
renv::restore(confirm = FALSE)
renv::restore()
renv::restore()
renv::diagnostics()
renv::restore(confirm = FALSE)
renv::restore(packages = c("readr" , "sf" , "dplyr" , "tidyr" , "ggplot2" , "ggsci" , "ggthemes" , "lubridate" , "stringr" , "spdep" , "Metrics" , "xgboost" , "pdp"))
renv::restore(packages = c("readr" , "sf" , "dplyr" , "tidyr" , "lubridate" , "stringr" , "Metrics" , "xgboost"))
install.packages("fansi")
install.packages("fansi")
renv::restore()
renv::restore()
renv::restore()
Sys.which("make")
writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
Sys.which("make")
Sys.which("make")
renv::restore()
install.packages("c:/path/to/downloaded/zip/file/colorspace_1.2-5.zip")
install.packages(file.choose())
renv::restore()
library(colorspace)
detach("package:colorspace", unload = TRUE)
remove.packages("colorspace", lib="~/Masterarbeit/renv/library/R-3.6/x86_64-w64-mingw32")
install.packages(file.choose())
renv::restore()
remove.packages("colorspace", lib="~/Masterarbeit/renv/library/R-3.6/x86_64-w64-mingw32")
install.packages(file.choose())
install.packages(file.choose())
renv::restore(packages = c("readr" , "dplyr" , "tidyr" , "lubridate" , "stringr" , "Metrics" , "xgboost"))
install.packages(file.choose())
renv::restore(packages = c("readr" , "dplyr" , "tidyr" , "lubridate" , "stringr" , "Metrics" , "xgboost"))
install.packages(file.choose())
renv::restore(packages = c("readr" , "dplyr" , "tidyr" , "lubridate" , "stringr" , "Metrics" , "xgboost"))
install.packages(file.choose())
renv::restore(packages = c("readr" , "dplyr" , "tidyr" , "lubridate" , "stringr" , "Metrics" , "xgboost"))
install.packages(file.choose())
renv::restore(packages = c("readr" , "dplyr" , "tidyr" , "lubridate" , "stringr" , "Metrics" , "xgboost"))
renv::restore(packages = c("astsa" , "gdalUtlis" , "lme4" , "ranger" , "raster" , "rgdal" , "sf" , "stars" , "zoo"))
install.packages(file.choose())
renv::restore(packages = c("astsa" , "gdalUtlis" , "lme4" , "ranger" , "raster" , "rgdal" , "sf" , "stars" , "zoo"))
renv::restore(packages = c("astsa" , "gdalUtlis" , "ranger" , "raster" , "rgdal" , "sf" , "stars" , "zoo"))
install.packages(file.choose())
install.packages(file.choose())
install.packages(file.choose())
renv::restore(packages = c("astsa" , "gdalUtlis" , "ranger" , "raster" , "rgdal" , "sf" , "stars" , "zoo"))
renv::restore(packages = c("ggplot2" , "ggthemes" , "ggsci"))
renv::restore(packages = c("ggplot2" , "ggthemes" , "ggsci"))
install.packages(file.choose())
install.packages(file.choose())
renv::restore(packages = c("ggplot2" , "ggthemes" , "ggsci"))
install.packages(file.choose())
renv::restore(packages = c("ggplot2" , "ggthemes" , "ggsci"))
renv::restore(packages = c("ggthemes" , "ggsci"))
renv::restore(packages = c("spdep"))
install.packages(file.choose())
install.packages(file.choose())
renv::restore(packages = c("spdep"))
install.packages(file.choose())
install.packages(file.choose())
renv::restore(packages = c("spdep"))
install.packages(file.choose())
# =====================================
# required packages
# =====================================
library(readr)
# =====================================
# required packages
# =====================================
library(readr)
# =====================================
# required packages
# =====================================
library(readr)
library(sf)
library(dplyr) ; library(tidyr)
# =====================================
# required packages
# =====================================
library(readr)
library(readr)
detach("package:readr", unload = TRUE)
library(readr)
library(renv)
library(readr)
renv::init()
# =====================================
# required packages
# =====================================
library(readr)
renv::init()
# =====================================
# required packages
# =====================================
library(readr)
library(xgboost)
# =====================================
# required packages
# =====================================
library(readr)
renv::restore()
# =====================================
# required packages
# =====================================
library(readr)
library(sf)
library(dplyr) ; library(tidyr)
library(ggplot2) ; library(ggsci) ; library(ggthemes)
library(lubridate) ; library(stringr)
library(spdep)
library(Metrics)
library(xgboost) ; library(pdp)
# =====================================
# load datasets
# =====================================
# training data
data_daily_raw <- read_csv("1_data/processed/cleaned/extracted/daily_scaled.csv")
# cross validation
{
in_filepath_CV <- list.files("1_data/processed/cleaned/extracted/" , "CV.shp$" , full.names = TRUE)
sites_CV <- st_read(in_filepath_CV) %>%
rename(Station_name = Station)
k_fold <- in_filepath_CV %>%
str_extract("\\d+-fold") %>% str_extract("\\d+") %>%
as.integer()
}
# implement the CV design in the training data
data_daily_raw <- data_daily_raw %>%
inner_join(sites_CV , by = "Station_name") %>%
select(-geometry)
# non-predictor columns
columns_nonpredictor <- c("Station_name" , "NO2" , "Type_of_zone" , "Type_of_station" ,
"Altitude" , "Canton_ID" , "Canton_name" , "X" , "Y" ,
"CV" , "spatial_CV" , "date")
# //////////////////////////////////////////////////////////////////////////
# naming the model
# //////////////////////////////////////////////////////////////////////////
SAT_product <- "TROPOMI"
# //////////////////////////////////////////////////////////////////////////
# data preparation for xgboost
# //////////////////////////////////////////////////////////////////////////
if(SAT_product == "OMI"){
data_daily <- data_daily_raw %>%
# OMI
select(-TROPOMI_NO2, -ends_with("12H")) %>%
# drop NA
drop_na() %>%
# date as DOY (numeric)
mutate(DOY = yday(date))
}else if(SAT_product == "TROPOMI") {
data_daily <- data_daily_raw %>%
# TROPOMI
select(-OMI_NO2, -ends_with("15H")) %>%
# drop NA
drop_na() %>%
# date as DOY (numeric)
mutate(DOY = yday(date))
}
# =====================================
# response
# =====================================
response_full <- data_daily %>%
select(NO2) %>%
unlist() %>% unname()
# =====================================
# predictor
# screening of important predictor variables
# =====================================
set.seed(123)
xgb_screen <- xgboost(
data = data_daily %>%
select(-all_of(columns_nonpredictor)) %>%
# random-value variables
mutate(R1 = runif(n()) ,
R2 = runif(n()) ,
R3 = runif(n()) ) %>%
# matrix
as.matrix(),
label = response_full ,
nrounds = 1000 ,
objective = "reg:squarederror" ,
verbose = FALSE ,  # silent
early_stopping_rounds = 5 # stop if no improvement for 5 consecutive trees
)
xgb_screen_importance <- xgb_screen %>%
xgb.importance(model = .) %>%
as_tibble()
# visualization
xgb_screen_importance %>%
mutate(class = ifelse(str_detect(Feature , "R[123]") , "Random" , "Predictor variables")) %>%
# reorder for visualization
mutate(Feature = factor(Feature , levels = Feature[order(Gain)])) %>%
# visualization
ggplot(aes(x = Feature , y = Gain , fill = class)) +
geom_bar(stat = "identity") +
coord_flip() +
scale_fill_lancet() +
labs(x = "Variables" , y = "Variable importance (Gain)" , fill = "" ,
title = "Screening of relevant predictor variables" ,
subtitle = sprintf("The variable importance of the gradient boosting machine (%s)" , SAT_product)) +
theme_bw() +
theme(axis.text.y = element_text(size = 4) , legend.position = "bottom")
# exclude not-important variables
included_var <- xgb_screen_importance %>%
filter(Gain > xgb_screen_importance %>%
filter(str_detect(Feature , "R[123]")) %>%
# the max importance of the random-value variables
summarize(Gain = max(Gain)) %>%
unlist) %>%
select(Feature) %>%
unlist %>% unname()
library(readr)
library(sf)
library(dplyr) ; library(tidyr)
library(ggplot2) ; library(ggsci) ; library(ggthemes)
library(lubridate) ; library(stringr)
library(spdep)
library(Metrics)
library(xgboost) ; library(pdp)
# =====================================
# predictor variables (as a matrix)
# =====================================
predictor_full <- data_daily %>%
select(all_of(included_var)) %>%
as.matrix()
# =====================================
# CV folds (as a list) (conventional random-split CV)
# =====================================
CV_folds <- data_daily %>%
mutate(rowID = 1:n()) %>%
select(CV , rowID) %>%
group_by(CV) %>%
group_map( ~ c(.x$rowID))
# =====================================
# create hyperparameter grid
# =====================================
# create hyperparameter grid
hyper_grid <- expand.grid(
# controls the learning rate
eta = c(.01, .05, .1, .3),
# tree depth
max_depth = c(1, 3, 5, 10),
# minimum number of observations required in each terminal node
min_child_weight = c(1, 3, 5, 7),
# percent of training data to sample for each tree
subsample = c(.65, .8, 1),
# percent of columns to sample from for each tree
colsample_bytree = c(.8, .9, 1) ,
optimal_trees = NA,               # a place to dump results
min_RMSE = NA,                    # a place to dump results
CV_R2 = NA                        # a place to dump results
)
pb <- txtProgressBar(min = 1 , max = nrow(hyper_grid) , style = 3 )
for(i in 1:nrow(hyper_grid)) {
# create parameter list
hyper_i <- list(
eta = hyper_grid$eta[i],
max_depth = hyper_grid$max_depth[i],
min_child_weight = hyper_grid$min_child_weight[i],
subsample = hyper_grid$subsample[i],
colsample_bytree = hyper_grid$colsample_bytree[i]
)
# reproducibility
set.seed(123)
# train model
xgb_grid <- xgb.cv(
params = hyper_i,
data = predictor_full,
label = response_full,
nrounds = 1000 ,
objective = "reg:squarederror" ,
folds = CV_folds ,
prediction = TRUE ,
verbose = FALSE ,  # silent
early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
# evaluation (add to hyper_grid data.frame)
# the number of iterations with the lowest CV-RMSE
hyper_grid$optimal_trees[i] <- which.min(xgb_grid$evaluation_log$test_rmse_mean)
# CV-RMSE
hyper_grid$min_RMSE[i] <- min(xgb_grid$evaluation_log$test_rmse_mean)
# CV-R2
hyper_grid$CV_R2[i] <- cor(response_full , xgb_grid$pred)^2
# progress bar
setTxtProgressBar(pb,i)
# clean environment
rm(hyper_i , xgb_grid)
}
rm(pb,i)
hyper_grid %>% arrange(-CV_R2)
# =====================================
# export grid search results
# =====================================
out_dirpath_hypergrid <- "3_results/output-data/model_daily/GBM_grid-search"
if(!dir.exists(out_dirpath_hypergrid)) dir.create(out_dirpath_hypergrid , recursive = TRUE)
hyper_grid %>%
write_csv(sprintf("%s/hyper_evaluation_full.csv" , out_dirpath_hypergrid))
library(keras)
keras::install_keras()
install.packages('Rcpp')
library(keras)
keras::install_keras()
renv::status()
renv::snapshot()
library(keras)
py_config()
library(reticulate)
py_config()
# test
mnist <- dataset_mnist()
